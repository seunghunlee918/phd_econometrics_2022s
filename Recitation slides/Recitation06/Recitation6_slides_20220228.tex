\documentclass[aspectratio=169]{beamer}

\mode<presentation>
\usetheme{Boadilla}
\definecolor{redback}{RGB}{140,0,0}
\definecolor{blue}{RGB}{30,90,205}
\definecolor{red}{RGB}{213,94,0}
\definecolor{green}{RGB}{0,128,0}
\setbeamercolor{title}{fg=redback}
\setbeamercolor{frametitle}{fg=redback}
\setbeamercolor{block title}{bg=redback, fg=white}
\setbeamercolor{block body}{bg=white}
\setbeamercolor{structure}{fg=redback}
\setbeamercolor{item projected}{fg=white}
\setbeamercolor{item}{fg=redback}
\setbeamercolor{subitem}{fg=redback}
\setbeamercolor{section in toc}{fg=redback}
\setbeamercolor{description item}{fg=redback}
\setbeamercolor{caption name}{fg=redback}
\setbeamercolor{button}{bg=redback, fg=white}
\setbeamercolor{caption name}{fg=redback}
\usepackage{graphics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{bbm}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{multirow, makecell}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{kotex}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage[scaled=0.92]{helvet}
\usepackage[default]{lato} %If I want a twist
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
\newenvironment{widedescription}{\description\addtolength{\itemsep}{10pt}}{\enddescription}
\hypersetup{
colorlinks=true,
linkcolor=redback,
filecolor=green, 
urlcolor=blue,
}
\beamertemplatenavigationsymbolsempty
\setbeamercolor{author in head/foot}{bg=white, fg=redback}
\setbeamercolor{title in head/foot}{bg=white, fg=redback}
\setbeamercolor{date in head/foot}{bg=white, fg=redback}
\setbeamercolor{section in head/foot}{bg=white, fg=redback}
\setbeamercolor{page number in head/foot}{bg=white, fg=redback}
\setbeamercolor{headline}{bg=redback}
\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.444444\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.222222\paperwidth,ht=2.25ex,dp=1ex,center]{page number in head/foot}%
            \usebeamerfont{page number in head/foot} \insertframenumber{} / \inserttotalframenumber
        \end{beamercolorbox}}%
        \vskip0pt%
    }

\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}{\leavevmode\leftskip=3em\rlap{\hskip-1.75em\inserttocsectionnumber.\inserttocsubsectionnumber}\inserttocsubsection\par}
\setbeamerfont{subsection in toc}{size=\footnotesize}

\newenvironment{transitionframe}{\setbeamercolor{background canvas}{bg=redback}\setbeamertemplate{footline}{} \begin{frame}}{\end{frame}}
\newcommand{\ROM}[1]
    {\MakeUppercase{\romannumeral #1}}
    \DeclareMathOperator*{\plim}{plim}
\makeatletter
\let\@@magyar@captionfix\relax
\makeatother


\title[Recitation 5 (Intro to Econometrics \ROM{2})]{Recitation 6: Estimating Time Series, POLS, RE} % Change this regularly
\author[]{Seung-hun Lee }
\institute[]{Columbia University \\ Introduction to Econometrics \ROM{2} Recitation}

\date[February 28th, 2022]{February 28th, 2022}

\begin{document}
\begin{frame}
\titlepage
\end{frame}


%%% Color slides for section headers: Use for colloquium version (The ones bewteen \iffals and \fi)

\begin{transitionframe}
  \begin{center}
         { \Huge \textcolor{white}{Time series regression}}
       \end{center}
\end{transitionframe}

\begin{frame}
\frametitle{Building blocks for time series regressions}
\begin{itemize}
\item Ergodicity theorem: For this, we want a sequence $\{Y_t\}$ that is strictly stationary and ergodic. Then if $E[Y_t]<\infty$, we have$ \frac{1}{T}\sum_{t=1}^Ty_t  \xrightarrow{p} E[Y_t]$
\item White noise: An error $e_t$ satisfying $E[e_t]=0, var[e_t]=\sigma^2$ and $\gamma(k)=0 \ \forall k\neq0$
\item Linear projection: $Y_t$ can be projected onto the set  of the history of the value of $Y$'s up until period $t-1$. Such projection is unique and so is the resulting projection error. Specifically, we can write $Y_t = \mathbb{P}_{t-1}(Y_t)+e_t$.  The resulting $e_t$ is serially uncorrelated (and thus WN) and $\mathbb{P}_{t-1}(Y_t)$ is covariance stationary provided that $\{Y_t\}$ is.
\item CLT: Let $\{y_t\}$ come from a dependent data satisfying strict stationarity and ergodicity. Specifically, let $y_t = \mu + e_t$ with $e_t$ being a white noise. then, as $T\to\infty$, the following holds, $\sqrt{T}\frac{\bar{y}-\mu}{\omega}\sim N(0,1)$
where $\omega^2= \sum_{j=-\infty}^\infty \gamma(j)=\gamma(0)+2\sum_{j=1}^\infty \gamma(j)$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Wold representation: Stationary process has linear representation}
\begin{itemize}
\item If we have a process $\{Y_t\}$ that is covariance stationary, we can represent this process as an infinite linear function of the projection errors
\item $Y_t$ can be written as  a linear function of the white noise errors and a deterministic part
\[
Y_t = \mu_t + \sum_{j=0}^\infty \psi_je_{t-j}
\]
If $\mu_t=\mu$, this is purely deterministic. 
\item This works for autoregressive and moving average processes
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MA(1): Shortest memory process}
\begin{itemize}
\item  MA(1) model has the following form
\[
y_t = \mu+ e_t + \theta e_{t-1}, \ e_t \sim (0,\sigma^2) \ \text{is WN}\ \forall t \ \text{(not necessarily normal)}
\]
\item We can calculate the key moment conditions as follows
\[
\begin{aligned}
E[y_t]&=\mu+E[e_t]+\theta E[e_{t-1}]=\mu\\
\gamma(0)&=var(y_t)=var(\mu+ e_t + \theta e_{t-1})\\
&=var(e_t + \theta e_{t-1}) \ (\because \ \text{constants do not affect dispersion})\\
&=var(e_t) + \theta^2 var(e_{t-1})=(1+\theta^2)\sigma^2\\
\gamma(1)&=cov(y_t,y_{t-1})=cov(\mu+ e_t + \theta e_{t-1},\mu+ e_{t-1} + \theta e_{t-2})\\
&=\theta cov(e_{t-1},e_{t-1})=\theta\sigma^2\\
\gamma(2)&=cov(y_t,y_{t-2})\\
&=cov(\mu+ e_t + \theta e_{t-1},\mu+ e_{t-2} + \theta e_{t-3})=0\\
\end{aligned}
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MA(q): Short-memory property is generalizable}
\begin{itemize}
\item For any $k>q$, the impact of the shock vanishes
\[
y_t = \mu+ \sum_{j=0}^q \theta_j e_{t-j}, \ e_t \sim (0,\sigma^2) \ \text{is WN}\ \forall t 
\]
\small{\[
\begin{aligned}
E[y_t]&=\mu\\
\gamma(0)&=var\left(\sum_{j=0}^q \theta_j e_{t-j}\right)=\left(\sum_{j=0}^q \theta_j\right)\sigma^2\\
\gamma(k)&=cov(y_t,y_{t-k})=cov\left(\mu+ \sum_{j=0}^q \theta_j e_{t-j},\mu+  \sum_{j=0}^q \theta_j e_{t-k-j}\right)\\
&=cov\left(\theta_ke_{t-k}+...+\theta_qe_{t-q}, \theta_0e_{t-k}+\theta_{q-k}e_{t-q}\right)=\left(\sum_{j=0}^{q-k}\theta_{j+k}\theta_j\right)\sigma^2\\
\gamma(k)&=0 \ \forall k>q
\end{aligned}
\]}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MA(q) can teach us about dynamic causal effect of shocks}
\begin{itemize}
\item With white noise errors, dynamic causal effects can be cleanly analyzed
\[
y_t = \sum_{j=0}^q \theta_j e_{t-j}
\]
\item We can find the impact of $e_{t-j}$ with $\frac{\partial y_t}{\partial e_{t-j}}=\theta_j$.
\item  The interpretation of $\theta_j$ is the dynamic multiplier of $e_{t-j}$ onto $y_t$. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{AR(p) process is much more perserverant}
\begin{itemize}
\item AR(p) has a Wold representation
\item If we take $t\to\infty$, we can write $y_t$ as a infinite sum of the white noise errors.
\[
\begin{aligned}
y_t &= e_t + \alpha e_{t-1} + \alpha^2 e_{t-2} + ...=\sum_{j=0}^\infty \alpha^je_{t-j}
\end{aligned}
\]
\item We effectively have MA($\infty$) process - a process that takes very long to die out.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{How persistent is the effect?}
\begin{itemize}
\item This is represented in these moment conditions.
\[
\begin{aligned}
\gamma(0)&=cov\left(\sum_{j=0}^\infty \alpha^j e_{t-j},\sum_{j=0}^\infty \alpha^j e_{t-j}\right)=\frac{\sigma^2}{1-\alpha^2}\\
\gamma(1)&=cov\left(\sum_{j=0}^\infty \alpha^j e_{t-j},\sum_{j=1}^\infty \alpha^{j-1} e_{t-j}\right)=\frac{\alpha\sigma^2}{1-\alpha^2}\\
\gamma(2)&=cov\left(\sum_{j=0}^\infty \alpha^j e_{t-j},\sum_{j=2}^\infty \alpha^{j-2} e_{t-j}\right)=\frac{\alpha^2\sigma^2}{1-\alpha^2}\\
\gamma(k)&=\frac{\alpha^k\sigma^2}{1-\alpha^2}
\end{aligned}
\]
\item As long as $|\alpha|<1$, this process does approximate to 0. Depending on the value of $\alpha$ the process can die off quickly, last long, or oscillate around 0. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Time series OLS is consistent but not unbiased}
\begin{itemize}
\item We want to estimate $\alpha$ in $y_t= \alpha_1y_{t-1}+e_t\  \ (e_t \sim (0,\sigma^2), |\alpha|<1)$
\item OLS estimate of $\alpha_1$ is $\hat{\alpha}_1=(X_t'X_t)^{-1}(X_t'y_t)$ where $X_t = [y_{t-1}]$
\item Consistency?  $X_t (=y_{t-1})$ and $e_t$ and $X_t'e_t$ are ergodic stationary. If mean of $y_t$ is finite, sample mean of $y_{t-1}e_t$ converges to 0. (Ergodicity theorem)
\item Bias? Strict exogeneity $E[e_t|X_1,...,X_T]=0$. is no longer guaranteed.
\[
\begin{aligned}
E[e_tX_{t-1}]&=E[e_ty_{t-2}]=0\\
E[e_tX_t]&=E[e_ty_{t-1}]=0\\
E[e_tX_{t+1}]&=E[e_ty_t]=\sigma^2(\neq0)\\
\end{aligned}
\]
\item Difference? Contemporary uncorrelatdness vs strict exogeneity
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{HAC standard errors are the standard}
\begin{itemize}
\item The error structure needs to take time dependence!
\item HAC: Has this variance structure
\[
\Omega=\sum_{j=-\infty}^\infty\Gamma_g(j)=\Gamma_g(0)+\sum_{j=1}^\infty\left(\Gamma_g(j)+\Gamma_g(j)'\right)
\]
where $g$ is the moment condition $E[X_te_t]=0$ and $\Gamma_g(k)=E[g_tg_{t-k}]$
\item By ergodicity $\Gamma_g(k)$ is eventually zero $\to$How much weight on long term variances?
\item Newey \& West: Proposes ($M=0.75\times T^{1/3}$)
\[
\widehat{\Omega}=\widehat{\Gamma}_g(0)+\sum_{j=1}^M \left(1-\frac{k}{M+1}\right)\left(\widehat{\Gamma}_g(j)+\widehat{\Gamma}_g(j)'\right)
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{How about GLS?}
\begin{itemize}
\item Recall that $D=E[ee'|X]$ can be represented by $D^{-1}=PP'$
\item In time series, with AR(1) error term $e_t = \rho e_{t-1}+u_t$ ($u_t$  is WN)
\scriptsize{\[
D=\begin{pmatrix}\gamma(0)  & \gamma(1) & \gamma(2) & ....\\ \gamma(1) & \gamma(0) & \gamma(1) & ....\\.... & ... & ... & ....\\ \gamma(T-1) & ... & ... & \gamma(0) \end{pmatrix}=\frac{\gamma(0)}{1-\rho^2}\begin{pmatrix} 1 & \rho & \rho^2 & ....\\ \rho & 1 & \rho & ....\\.... & ... & ... & ....\\ \rho^{T-1} & ... & ... & 1\end{pmatrix}, \ \  P'=\frac{1}{\sigma_u}\begin{pmatrix} \sqrt{1-\rho^2} & 0 & 0 & ...\\ -\rho & 1 & 0 & ....\\.... & ... & ... & ....\\0 & ... & -\rho & 1\end{pmatrix}
\]}\normalsize
\item  Effectively, we transform (or quasi-difference) the DGP $y_t = x_t'\beta+e_t$ by
\[
y_{t}-\rho y_{t-1} = (x_t-\rho x_{t-1})'\beta + \underbrace{e_t-\rho e_{t-1}}_{u_t\to\text{spherical!}}
\]
\item we need to check whether a stronger condition $E[(x_t-\rho x_{t-1})u_t]=0$ holds.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model selection: How many lags?}
\begin{itemize}
\item \textbf{Information criterion}: Gives the number that sums the measure of fitness of the model and the penalty for a complex model. 
\item They are calculated as
\[
IC(k) = \underbrace{\log\hat{\sigma}^2}_{\text{Penalty for lack of fit}}+\underbrace{k\frac{C_T}{T}}_{\text{Penalty for complex model}}
\]
where there are $k$ total regressors
\item Akaike information criterion: $C_t= 2$
\item Bayesian information criterion: $C_T=\log{T}$
\end{itemize}
\end{frame}

\begin{transitionframe}
  \begin{center}
         { \Huge \textcolor{white}{Panel Regression}}
       \end{center}
\end{transitionframe}


\begin{frame}
\frametitle{Panel deals with entity/time-fixed unobserved factors}
\begin{itemize}
\item Let $y$ and $x=(x_1,x_2,...,x_k)$ be the observable factors. 
\item $\alpha$: Unobservable random variable incorporated into the DGP additively. 
\item Then, we can write $E[y|x,\alpha]$ as 
\[
E[y|x,\alpha] = x'\beta+\alpha
\]
\item $\alpha$ independent from $x$? $\beta$ is consistently estimable. 
\item $\alpha$ correlated with $x$? Still a chance if $\alpha$ fixed across time/entities and have panel data
\item Most discussion of panel data in class will focus on $\alpha$ representing an unobservable trait inherent for individuals or a common trend in a particular period
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Setting up the panel regression}
\begin{itemize}
\item Our data generating process is
\[
y_{it}= x_{it}'\beta+ v_{it} \iff y_i = X_i\beta+v_i
\]
where $x_{it}\in\mathbb{R}^k, y_{it}\in\mathbb{R}$, and $y_i \in\mathbb{R}^T, X_i \in\mathbb{R}^{T\times k}$
\item $v_{it}$: `composite' error term
\begin{itemize}
\item One-way FE: $v_{it}= c_i + e_{it}$
\item Two-way FE (TWFE): $v_{it}= c_i + \delta_t + e_{it}$
\item Interacted FE (IFE): $v_{it}= \lambda_if_t + e_{it}$
\end{itemize}
\item The assumption on $c_i$ determines the method of panel regression we will use
\item The assumptions on $e_{it}$ the degree of exogeneity we are willing to assume.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Types of panel regression}
\begin{itemize}
\item There are three types of estimators we study in (static) panel regression, determined by assumptions on $E[c_i|X_i]$
\end{itemize}
\begin{block}{POLS, RE, FE}
\begin{itemize}
\item Is $c_i$ constant $\forall i$? Then \textbf{pooled OLS (POLS) estimator} is consistent and efficient
\item Is $c_i$ latent (in that it is unobserved and possibly different for each $i$) but uncorrelated with $X_i$? Then POLS is consistent but inefficient. We use \textbf{random effects (RE) GLS estimator} here
\item Is $c_i$ latent and possibly correlated with $X_i$? Then POLS and RE are inconsistent. We use \textbf{fixed effects (FE) estimator} - which are within estimation (WE), least squares dummy variables (LSDV), and in case we have $T=2$, first-differencing (FD)
\end{itemize} 
\end{block}
\end{frame}

\begin{frame}
\frametitle{Types of exogeneity}
\begin{itemize}
\item They have important bearings on consistency/unbiasedness
\end{itemize}
\begin{block}{Strict and sequential exogeneity (predetermined regressors)}
\begin{itemize}
\item \textbf{Strict exogeneity}: $E[e_{it}|x_{i1},...,x_{iT},c_i]=0$. Or that the past, current, or future values of $x_{it}$ cannot predict $e_{it}$
\begin{itemize}
\item Weaker version: $E[x_{is}e_{it}]=0$ for $s=1,...,T$. This implies that $e_{it}$ cannot be predicted by $x_{is}$
\end{itemize}
\item  \textbf{Sequential exogeneity}: $E[e_{it}|x_{i1},...,x_{it},c_i]=0$ for each $t$. Past and present value of $x_{it}$ cannot predict $e_{it}$. It is silent on whether future values of $x_{it}$ predicts $e_{it}$
\end{itemize} 
\end{block}
\end{frame}

\begin{frame}
\frametitle{Pooled OLS: $c_i$ is a constant and just another overall intercept!}
\begin{itemize}
\item The data generating process is written
\[
y_{it} = x_{it}'\beta + c + e_{it}
\]
\item If we assume that $E[x_{it}e_{it}]=0$, we end up with the following pooled OLS estimate 
\[
\begin{aligned}
\hat{\beta}_{POLS}&=\left(\sum_{i=1}^n\sum_{t=1}^Tx_{it}x_{it}'\right)^{-1}\left(\sum_{i=1}^n\sum_{t=1}^Tx_{it}y_{it}\right)\\
&=\left(\sum_{i=1}^nX_{i}'X_{i}\right)^{-1}\left(\sum_{i=1}^nX_{i}'y_{i}\right)\\
\end{aligned}
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Pooled OLS properties are much like OLS}
\begin{itemize}
\item We can find unbiasedness, consistency, variance, and asymptotics
\[
\begin{aligned}
E[\hat{\beta}_{POLS}|X_i]&= \beta + \left(\sum_{i=1}^n\sum_{t=1}^Tx_{it}x_{it}'\right)^{-1}\left(\sum_{i=1}^n\sum_{t=1}^Tx_{it}E[e_{it}|x_{i1},....,x_{iT},c]\right)=\beta\\
\plim\hat{\beta}_{OLS}&=\beta+\plim\left(\sum_{i=1}^n\sum_{t=1}^Tx_{it}x_{it}'\right)^{-1}\left(\sum_{i=1}^n\sum_{t=1}^Tx_{it}e_{it}\right) = \beta\\
\widehat{V}_{\hat{\beta}_{POLS}}&= (X'X)^{-1}\left(\sum_{i=1}^n X_i'\hat{e}_i\hat{e}_i'X_i\right)(X'X)^{-1}\\
\sqrt{nT}(\hat{\beta}_{POLS}-\beta)&\xrightarrow{d}N(0,V_\beta)
\end{aligned}
\]
\item Things are not clear cut if $c_i$ can be correlated with $x_{it}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Random effect: Treat $c_i$ like another error term}
\begin{itemize}
\item We assume that $c_i$ is latent, but uncorrelated with $x_i$
\end{itemize}
\begin{block}{Random effects assumptions}
\begin{itemize}
\item[RE1] We assume $E[c_i|X_i]=0, E[c_i]=0$ and strict exogeneity $E[e_{it}|X_i,c_i]=0$
\item[RE2] $\text{rank}\left(E [X_i' \Omega^{-1}X_i]\right)=k$ (full column rank)
\item[RE3] Conditionally spherical variance matrix: $E[e_ie_i'|X_i,c_i]=\sigma_e^2 I_T$ and $E[c_i^2|X_i]=\sigma_c^2$
\end{itemize}
\end{block}
\begin{itemize}
\item The covariance of $e_{it}$ and $v_{it}$ is
\[
E[e_{it}e_{is}]=\begin{cases} 0 & t\neq s \\ \sigma_e^2 & t=s \end{cases},\ E[v_{it}v_{is}]=\begin{cases} \sigma_c^2 & t\neq s \\ \sigma_e^2+\sigma_c^2 & t=s \end{cases}
\]
\item Composite error term $v_{it}$ has a serial correlation! $\to$ GLS gives efficient estimates
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Variance-covariance matrix of the $v_{it}$}
\begin{itemize}
\item We can write
\[
v_i = 1_Tc_i + u_i
\]
\item Using this notation, we can define
\[
\begin{aligned}
E[v_iv_i']&=E\left[(1_Tc_i + e_i)(1_Tc_i + e_i)'\right]=E[1_T1_T'c_i^2+e_ie_i']\\
&=\sigma_c^21_T1_T'+\sigma_e^2I_T\\
\end{aligned}
\]
\item  $E[v_iv_i']$ has the following matrix representation
\[
E[v_iv_i']=\begin{pmatrix}\sigma_c^2+\sigma_e^2 & \sigma_c^2 & ... &.... \\ \sigma_c^2 & \sigma_c^2+\sigma_e^2 &...&...\\ ...&...&...&...\\ ... &...&\sigma_c^2&\sigma_c^2+\sigma_e^2\end{pmatrix} =\Omega
\]

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{GLS estimator for RE}
\begin{itemize}
\item Using $\Omega$, we can write
\[
\hat{\beta}_{RE} = \left(\sum_{i=1}^n X_i'\widehat{\Omega}^{-1}X_i\right)^{-1}\left(\sum_{i=1}^n X_i'\widehat{\Omega}^{-1}y_i\right)
\]
\item Sample variance of $\left(\sum_{i=1}^n X_i'\widehat{\Omega}^{-1}X_i\right)^{-1}$. 
\item $\widehat{\Omega}$ can be obtained by estimating each component of the $\Omega$ matrix. $\sigma_e^2$ can be obtained by the unbiased estimate of $E[e_{it}^2]$ and $\sigma_c^2$ from that for $E[v_{it}v_{is}]$ for $t\neq s$. 
\[
\begin{aligned}
\hat{\sigma}_e^2&=\frac{1}{nT-k}\sum_{i=1}^n\sum_{t=1}^T \hat{e}_{it}^2\\
\hat{\sigma}_c^2&=\frac{1}{nT(T-1)/2-k}\sum_{i=1}^n\sum_{t=1}^T\sum_{s=t+1}^T \hat{v}_{it}\hat{v}_{is}\\
\end{aligned}
\]

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{RE GLS as a quasi-demeaned estimator}
\begin{itemize}
\item Note that 
\[
E[v_iv_i']=\sigma_c^21_T1_T'+\sigma_e^2I_T=T\sigma_c^21_T(\underbrace{1_T'1_T}_{T})^{-1}1_T'+\sigma_e^2I_T=T\sigma_c^2P_{1_T}+\sigma_e^2I_T
\]
\item So the square root matrix of $\Omega$, which I write $P$ (s.t. $PP'=\Omega^{-1}$) is
\[
P'=\frac{1}{\sigma_e}[I_T - \rho P_T] \ \ \left(\rho = 1-\sqrt{\frac{\sigma_e^2}{T\sigma_c^2+\sigma_e^2}}\right)
\]
\item Thus, the transformed data generating process $\tilde{y}_{it}=\tilde{x}'_{it}\beta+\tilde{v}_{it}$, where $\tilde{y}_{it}=y_{it}-\rho\bar{y}_i$ has a spherical variances involving $\tilde{v}_{it}$ term.

\end{itemize}
\end{frame}
%%%%%%%%%%%
\end{document}
